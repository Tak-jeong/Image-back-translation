{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from typing import Any\n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "import torch\n",
    "from lightning import LightningModule\n",
    "from torchmetrics import MaxMetric, MeanMetric\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"/home/tak/IBT/Image-back-translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixAugLitModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "        num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.net = net\n",
    "\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # self.train_acc = Accuracy(task=\"multilabel\", num_labels=num_classes, average='micro')\n",
    "        # self.val_acc = Accuracy(task=\"multilabel\", num_labels=num_classes, average='micro')\n",
    "        # self.test_acc = Accuracy(task=\"multilabel\", num_labels=num_classes, average='micro')\n",
    "\n",
    "        self.train_acc_top1 = Accuracy(task=\"multilabel\", num_labels=num_classes, top_k=1, average='micro')\n",
    "        self.train_acc_top5 = Accuracy(task=\"multilabel\", num_labels=num_classes, top_k=5, average='micro')\n",
    "\n",
    "        self.val_acc_top1 = Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=1, average='micro')\n",
    "        self.val_acc_top5 = Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=5, average='micro')\n",
    "\n",
    "        self.test_acc_top1 = Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=1, average='micro')\n",
    "        self.test_acc_top5 = Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=5, average='micro')\n",
    "        print(f\"num_classes: {num_classes}\")\n",
    "        \n",
    "\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        self.val_acc_best = MaxMetric()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        # by default lightning executes validation step sanity checks before training starts,\n",
    "        # so it's worth to make sure validation metrics don't store results from these checks\n",
    "        self.val_loss.reset()\n",
    "        self.val_acc_top1.reset()\n",
    "        self.val_acc_top5.reset()\n",
    "        self.val_acc_best.reset()\n",
    "\n",
    "    def model_step(self, batch: Any):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # For multi-label classification\n",
    "        preds = torch.sigmoid(logits)\n",
    "        \n",
    "        return loss, preds, y\n",
    "\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # Convert soft labels to hard labels for accuracy calculation\n",
    "        hard_targets = torch.zeros_like(targets)\n",
    "        hard_targets[torch.arange(targets.size(0)), targets.argmax(1)] = 1\n",
    "\n",
    "        # update and log metrics\n",
    "        self.train_loss(loss)\n",
    "\n",
    "        self.train_acc_top1(preds, hard_targets)\n",
    "        self.train_acc_top5(preds, hard_targets)\n",
    "        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/acc_top1\", self.train_acc_top1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/acc_top5\", self.train_acc_top5, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # return loss or backpropagation will fail\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.val_loss(loss)\n",
    "\n",
    "        _, targets_indices = torch.max(targets, dim=1)\n",
    "        self.val_acc_top1(preds, targets_indices)\n",
    "        self.val_acc_top5(preds, targets_indices)\n",
    "        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/acc_top1\", self.val_acc_top1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/acc_top5\", self.val_acc_top5, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        acc = self.val_acc_top1.compute()  # get current val acc\n",
    "        self.val_acc_best(acc)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.test_loss(loss)\n",
    "\n",
    "        _, preds_indices = torch.max(preds, dim=1)\n",
    "        _, targets_indices = torch.max(targets, dim=1)\n",
    "\n",
    "\n",
    "        self.test_acc_top1(preds_indices, targets_indices)\n",
    "        self.test_acc_top5(preds_indices, targets_indices)\n",
    "        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test/acc_top1\", self.test_acc_top1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test/acc_top5\", self.test_acc_top5, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n",
    "\n",
    "        Examples:\n",
    "            https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#configure-optimizers\n",
    "        \"\"\"\n",
    "        optimizer = self.hparams.optimizer(params=self.parameters())\n",
    "        if self.hparams.scheduler is not None:\n",
    "            scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val/loss\",\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "from src.data.components import MixAugDataset, CMIAImageFolder\n",
    "\n",
    "import torch\n",
    "from lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "class ConcatDataModule(LightningDataModule):\n",
    "    def __init__(self, root_path: str, train_dir: str, aug_dir: str, val_dir: str, batch_size: int, num_classes=1000, concat=True, aug_num=1):\n",
    "        super().__init__()\n",
    "        self.root_path = root_path\n",
    "        self.train_dir = os.path.join(self.root_path, train_dir)\n",
    "        self.aug_dir = os.path.join(self.root_path, aug_dir)\n",
    "        self.val_dir = os.path.join(self.root_path, val_dir)\n",
    "        # self.test_dir = os.path.join(self.root_path, 'test')\n",
    "\n",
    "        self.concat = concat\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.train_transform = transforms.Compose([\n",
    "                                transforms.Resize((512, 512)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                    std=[0.229, 0.224, 0.225]),\n",
    "                                # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n",
    "                                                    ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def one_hot(self, x, num_classes, on_value=1., off_value=0.):\n",
    "        x = x.long().view(-1, 1)\n",
    "        return torch.full((x.size()[0], num_classes), off_value, device=x.device).scatter_(1, x, on_value)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, targets = list(zip(*batch))\n",
    "        targets = self.one_hot(torch.tensor(targets, dtype=torch.int64), self.num_classes)\n",
    "        images = torch.stack(images)\n",
    "        return images, targets\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split the dataset into train, val, and test sets\n",
    "        # Create instances of the CustomDataset for each split\n",
    "        self.original_dataset = datasets.ImageFolder(self.train_dir, self.train_transform)\n",
    "        self.aug_dataset = datasets.ImageFolder(self.aug_dir, self.train_transform)\n",
    "\n",
    "        if self.concat:\n",
    "            self.train_dataset = ConcatDataset([self.original_dataset, self.aug_dataset])\n",
    "        else:\n",
    "            self.train_dataset = self.original_dataset\n",
    "\n",
    "        self.val_dataset = datasets.ImageFolder(self.val_dir, self.val_transform)\n",
    "        # self.test_dataset = datasets.ImageFolder(self.test_dir, self.val_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True, collate_fn=self.val_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data_module = ConcatDataModule(root_path='/data2/tak/1000way', train_dir='train_10percent', aug_dir='train_10percent_IBT', val_dir='val_formatted', batch_size=128, num_classes=1000, concat=False)\n",
    "concat_data_module.setup()\n",
    "val_loader = concat_data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tak/miniconda3/envs/IBT/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 1000\n",
      "device: cuda:7\n"
     ]
    }
   ],
   "source": [
    "check_point_path = \"/nvme_data1/tak/wandb/concat/runs/2023-10-31_14-36-46/checkpoints/epoch_012.ckpt\"\n",
    "model = MixAugLitModule.load_from_checkpoint(check_point_path)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = Accuracy(task=\"multiclass\", num_classes=1000, top_k=1, average='micro').to(device)\n",
    "acc_top5 = Accuracy(task=\"multiclass\", num_classes=1000, top_k=5, average='micro').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [06:25<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.005966499004789325\n",
      "Validation Top-1 Accuracy: 0.12479999661445618\n",
      "Validation Top-5 Accuracy: 0.28859999775886536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 검증 과정\n",
    "val_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = model.criterion(logits, y)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # 예측 결과를 확률로 변환\n",
    "        preds = torch.sigmoid(logits)\n",
    "\n",
    "        # 타겟 인덱스 추출\n",
    "        _, targets_indices = torch.max(y, dim=1)\n",
    "\n",
    "        # 정확도 업데이트\n",
    "        acc_top1(preds, targets_indices)\n",
    "        acc_top5(preds, targets_indices)\n",
    "\n",
    "# 평균 손실 계산\n",
    "val_loss /= len(val_loader)\n",
    "\n",
    "# 성능 메트릭 출력\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Top-1 Accuracy: {acc_top1.compute()}\")\n",
    "print(f\"Validation Top-5 Accuracy: {acc_top5.compute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
