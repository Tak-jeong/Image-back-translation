{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4800aad6c664d9583b43812cc61291c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: prompt: Union[str, List[str]] = None\n",
      "prompt_2: prompt_2: Union[str, List[str], NoneType] = None\n",
      "height: height: Union[int, NoneType] = None\n",
      "width: width: Union[int, NoneType] = None\n",
      "num_inference_steps: num_inference_steps: int = 50\n",
      "timesteps: timesteps: List[int] = None\n",
      "denoising_end: denoising_end: Union[float, NoneType] = None\n",
      "guidance_scale: guidance_scale: float = 5.0\n",
      "negative_prompt: negative_prompt: Union[str, List[str], NoneType] = None\n",
      "negative_prompt_2: negative_prompt_2: Union[str, List[str], NoneType] = None\n",
      "num_images_per_prompt: num_images_per_prompt: Union[int, NoneType] = 1\n",
      "eta: eta: float = 0.0\n",
      "generator: generator: Union[torch._C.Generator, List[torch._C.Generator], NoneType] = None\n",
      "latents: latents: Union[torch.FloatTensor, NoneType] = None\n",
      "prompt_embeds: prompt_embeds: Union[torch.FloatTensor, NoneType] = None\n",
      "negative_prompt_embeds: negative_prompt_embeds: Union[torch.FloatTensor, NoneType] = None\n",
      "pooled_prompt_embeds: pooled_prompt_embeds: Union[torch.FloatTensor, NoneType] = None\n",
      "negative_pooled_prompt_embeds: negative_pooled_prompt_embeds: Union[torch.FloatTensor, NoneType] = None\n",
      "ip_adapter_image: ip_adapter_image: Union[PIL.Image.Image, numpy.ndarray, torch.FloatTensor, List[PIL.Image.Image], List[numpy.ndarray], List[torch.FloatTensor], NoneType] = None\n",
      "ip_adapter_image_embeds: ip_adapter_image_embeds: Union[List[torch.FloatTensor], NoneType] = None\n",
      "output_type: output_type: Union[str, NoneType] = 'pil'\n",
      "return_dict: return_dict: bool = True\n",
      "cross_attention_kwargs: cross_attention_kwargs: Union[Dict[str, Any], NoneType] = None\n",
      "guidance_rescale: guidance_rescale: float = 0.0\n",
      "original_size: original_size: Union[Tuple[int, int], NoneType] = None\n",
      "crops_coords_top_left: crops_coords_top_left: Tuple[int, int] = (0, 0)\n",
      "target_size: target_size: Union[Tuple[int, int], NoneType] = None\n",
      "negative_original_size: negative_original_size: Union[Tuple[int, int], NoneType] = None\n",
      "negative_crops_coords_top_left: negative_crops_coords_top_left: Tuple[int, int] = (0, 0)\n",
      "negative_target_size: negative_target_size: Union[Tuple[int, int], NoneType] = None\n",
      "clip_skip: clip_skip: Union[int, NoneType] = None\n",
      "callback_on_step_end: callback_on_step_end: Union[Callable[[int, int, Dict], NoneType], NoneType] = None\n",
      "callback_on_step_end_tensor_inputs: callback_on_step_end_tensor_inputs: List[str] = ['latents']\n",
      "kwargs: **kwargs\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "\n",
    "# __call__ 메서드의 파라미터 확인\n",
    "params = inspect.signature(pipe.__call__).parameters\n",
    "for param_name, param in params.items():\n",
    "    print(f\"{param_name}: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# dataß\n",
    "caption_dir = Path(\"/mnt/nas65/Dataset/ImageNet1K/ILSVRC/Data/CLS-LOC/train_csv\")\n",
    "cls_mapping = pd.read_csv(\"./map_cls.txt\", sep=' ', header=None, names=['cls_name','cls_num', 'cls_id'])\n",
    "# model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 사전학습된 모델 로드 (예: Google News dataset의 Word2Vec 모델)\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# 캡션과 비교할 단어\n",
    "target_word = 'cat'\n",
    "\n",
    "# 캡션 내의 단어들 (예시)\n",
    "caption_words = ['dog', 'mouse', 'cat', 'pet']\n",
    "\n",
    "# 각 단어와 타겟 단어 사이의 유사도 계산\n",
    "similarities = {word: model.similarity(target_word, word) for word in caption_words if word in model.key_to_index}\n",
    "\n",
    "# 가장 유사도가 높은 단어 찾기\n",
    "most_similar_word = max(similarities, key=similarities.get)\n",
    "\n",
    "print(\"가장 유사한 단어:\", most_similar_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 캡션 데이터 로드\n",
    "path = Path('/mnt/nas65/Dataset/ImageNet1K/ILSVRC/Data/CLS-LOC/train_csv/n01440764.csv')\n",
    "csvfile = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile['caption'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_mapping = Path('./map_cls.txt')\n",
    "cls_mapping = pd.read_csv(cls_mapping, sep=' ',index_col='cls_name', header=None, names=['cls_name','cls_num', 'cls_id'])\n",
    "cls_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n",
      "Original sentence: I enjoy watching football and basketball.\n",
      "Modified sentence: I enjoy watching football and basketballsoccer\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 초기화\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Working on {device}\")\n",
    "model.eval()\n",
    "\n",
    "# 문장과 타겟 단어 설정\n",
    "sentence = \"I enjoy watching football and basketball.\"\n",
    "target_word = \"soccer\"\n",
    "\n",
    "# 문장을 토큰화하고 BERT 입력 형식으로 변환\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "# 모델을 통해 임베딩 계산\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    hidden_states = outputs.last_hidden_state.squeeze()\n",
    "\n",
    "# 타겟 단어의 임베딩을 계산\n",
    "target_tokens = tokenizer.tokenize(target_word)\n",
    "target_indexed_tokens = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "target_tokens_tensor = torch.tensor([target_indexed_tokens]).to(device)\n",
    "with torch.no_grad():\n",
    "    target_outputs = model(target_tokens_tensor)\n",
    "    target_hidden_states = target_outputs.last_hidden_state.squeeze()\n",
    "    target_hidden_state = target_hidden_states.mean(dim=0)\n",
    "\n",
    "# 유사도 계산 (코사인 유사성)\n",
    "cosine_similarities = torch.nn.functional.cosine_similarity(hidden_states, target_hidden_state.unsqueeze(0), dim=1)\n",
    "\n",
    "# 가장 유사한 단어 찾기\n",
    "most_similar_token_idx = torch.argmax(cosine_similarities).item()\n",
    "most_similar_word = tokens[most_similar_token_idx]\n",
    "\n",
    "# 원래 문장에서 단어 대치\n",
    "new_sentence = sentence.replace(most_similar_word, target_word)\n",
    "print(\"Original sentence:\", sentence)\n",
    "print(\"Modified sentence:\", new_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tak/miniconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class CSVModifier:\n",
    "    def __init__(self, model_name='bert-base-uncased', caption_dir=\"/mnt/nas65/Dataset/ImageNet1K/ILSVRC/Data/CLS-LOC/train_csv\", save_dir=\"/path/to/save\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.caption_dir = caption_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.mapping = pd.read_csv(\"/home/tak/IBT/Image-back-translation/notebooks/submaterial/map_cls.txt\", sep=' ', header=None, names=['cls_name', 'cls_num', 'cls_id'])\n",
    "        self.mapping_dict = {key: val.replace(\"_\",\" \") for key, val in zip(self.mapping['cls_name'], self.mapping['cls_id'])}\n",
    "\n",
    "    def modify_all_csvs(self):\n",
    "        for csv_file in os.listdir(self.caption_dir):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                self.modify_csv(csv_file)\n",
    "\n",
    "    # def modify_csv(self, csv_file):\n",
    "    #     df = pd.read_csv(os.path.join(self.caption_dir, csv_file), sep=r\",(?:(?!\\s)+(?!')+(?!$))\", engine='python')\n",
    "    #     df['modified_caption'] = self.replace_words(df['caption'].tolist(), csv_file.replace('.csv', ''))\n",
    "\n",
    "    #     df.to_csv(os.path.join(self.save_dir, f\"{csv_file.replace('.csv', '')}_modified.csv\"), index=False)\n",
    "\n",
    "    def modify_csv(self, csv_file):\n",
    "        df = pd.read_csv(os.path.join(self.caption_dir, csv_file), sep=r\",(?:(?!\\s)+(?!')+(?!$))\", engine='python')\n",
    "        df['modified_caption'] = self.replace_words(df['caption'].tolist(), csv_file.replace('.csv', ''))\n",
    "\n",
    "        # Save the DataFrame as a JSON file\n",
    "        json_file_path = os.path.join(self.save_dir, f\"{csv_file.replace('.csv', '')}_modified.json\")\n",
    "        df.to_json(json_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    # def replace_words(self, texts, class_folder):\n",
    "    #     class_embedding = self.get_class_embedding(class_folder)\n",
    "    #     tokenized = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    #     tokenized.to(self.device)\n",
    "    #     with torch.no_grad():\n",
    "    #         outputs = self.model(**tokenized)\n",
    "    #         hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    #     modified_texts = []\n",
    "    #     class_name = self.mapping_dict.get(class_folder, class_folder)  # 클래스 폴더명에 해당하는 클래스 이름 가져오기\n",
    "    #     for idx, text in enumerate(texts):\n",
    "    #         hidden_state = hidden_states[idx]\n",
    "    #         similarities = torch.nn.functional.cosine_similarity(hidden_state, class_embedding.unsqueeze(0), dim=1)\n",
    "    #         most_similar_token_idx = torch.argmax(similarities).item()\n",
    "\n",
    "    #         most_similar_word_id = tokenized['input_ids'][idx][most_similar_token_idx].unsqueeze(0)\n",
    "    #         most_similar_word = self.tokenizer.convert_ids_to_tokens(most_similar_word_id)[0]\n",
    "\n",
    "    #         # 가장 유사한 단어를 클래스 이름으로 대체\n",
    "    #         text = text.replace(most_similar_word, class_name)\n",
    "    #         modified_texts.append(text)\n",
    "\n",
    "    #     return modified_texts\n",
    "    \n",
    "    def replace_words(self, texts, class_folder):\n",
    "        class_embedding = self.get_class_embedding(class_folder)\n",
    "        tokenized = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        tokenized.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        modified_texts = []\n",
    "        class_name = self.mapping_dict.get(class_folder, class_folder)\n",
    "        for idx, text in enumerate(texts):\n",
    "            # POS tagging to filter nouns\n",
    "            words = word_tokenize(text)\n",
    "            pos_tags = pos_tag(words)\n",
    "            nouns = [word for word, tag in pos_tags if tag.startswith('NN')]\n",
    "\n",
    "            hidden_state = hidden_states[idx]\n",
    "            similarities = torch.nn.functional.cosine_similarity(hidden_state, class_embedding.unsqueeze(0), dim=1)\n",
    "            token_ids = tokenized['input_ids'][idx]\n",
    "            token_texts = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "            # Filter similarities for nouns only\n",
    "            noun_indices = [i for i, token in enumerate(token_texts) if token in nouns]\n",
    "            if not noun_indices:\n",
    "                modified_texts.append(text)\n",
    "                continue\n",
    "\n",
    "            noun_similarities = similarities[noun_indices]\n",
    "            most_similar_noun_idx = torch.argmax(noun_similarities).item()\n",
    "            most_similar_noun = token_texts[noun_indices[most_similar_noun_idx]]\n",
    "\n",
    "            # Replace the most similar noun with the class name\n",
    "            text = text.replace(most_similar_noun, class_name)\n",
    "            modified_texts.append(text)\n",
    "\n",
    "        return modified_texts\n",
    "\n",
    "\n",
    "\n",
    "    def get_class_embedding(self, class_folder):\n",
    "        class_name = self.mapping_dict.get(class_folder, class_folder)  # Assume class_folder is a valid key\n",
    "        tokens_tensor = torch.tensor([self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(class_name))]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor)\n",
    "            class_embedding = outputs.last_hidden_state.squeeze().mean(dim=0)\n",
    "        return class_embedding\n",
    "\n",
    "# Usage example\n",
    "modifier = CSVModifier(save_dir=\"/home/tak/IBT/Image-back-translation/notebooks/submaterial/aligned_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier.modify_all_csvs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
